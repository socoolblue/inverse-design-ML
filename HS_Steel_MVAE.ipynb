{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time, math\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(777)\n",
    "\n",
    "xy = np.loadtxt(\"X8 data input path\", delimiter=',', dtype=np.float32)\n",
    "\n",
    "xy_data = xy.reshape(-1,11)\n",
    "np.random.shuffle(xy_data)\n",
    "\n",
    "data_size = len(xy_data)\n",
    "\n",
    "input_dim = 8\n",
    "z_dim = 5\n",
    "output_dim = 16\n",
    "\n",
    "x_data_training = xy_data[:, 0:-3]\n",
    "y_data_training = xy_data[:, -3:-1]\n",
    "y_1 = xy_data[:, -1]\n",
    "y_1=tf.one_hot(y_1, 3)\n",
    "\n",
    "batch_size = tf.placeholder(tf.int32, name = 'batch_size')\n",
    "X = tf.placeholder(tf.float32, shape=[None, input_dim])\n",
    "learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "\n",
    "W1 = tf.get_variable(name='weight1', shape=[input_dim, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name='weight2', shape=[64, 32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(name='weight3', shape=[32, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4_u = tf.get_variable(name='weight4_u', shape=[16, z_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1 = tf.Variable(tf.random_normal([64]), name='bias1')\n",
    "b2 = tf.Variable(tf.random_normal([32]), name='bias2')\n",
    "b3 = tf.Variable(tf.random_normal([16]), name='bias3')\n",
    "b4_u = tf.Variable(tf.random_normal([z_dim]), name='bias4_u')\n",
    "\n",
    "L2 = tf.matmul(X, W1) + b1\n",
    "L3 = tf.matmul(L2, W2) + b2\n",
    "L4 = tf.matmul(L3, W3) + b3\n",
    "L5_u = tf.matmul(L4, W4_u) + b4_u\n",
    "\n",
    "one = tf.ones([batch_size,1])\n",
    "one = tf.reshape(one, [batch_size])\n",
    "\n",
    "Sigma_square_mat = tf.concat([tf.transpose([tf.exp(L5_u[:,2])]),tf.transpose([tf.exp(L5_u[:,3])]), \n",
    "                           tf.transpose([tf.exp(L5_u[:,3])]),tf.transpose([tf.exp(L5_u[:,4])])], 1)\n",
    "\n",
    "Sigma_square_mat = tf.reshape(Sigma_square_mat, shape=[batch_size, 2, 2])\n",
    "Sigma_mat = tf.sqrt(Sigma_square_mat)\n",
    "\n",
    "eps = tf.random_normal(([batch_size, 2, 1]), 0, 1, dtype=tf.float32)\n",
    "\n",
    "eps = tf.reshape(tf.matmul(Sigma_mat, eps),[batch_size,2])\n",
    "\n",
    "Z = tf.add(L5_u[:,:2], eps) \n",
    "Z_ = tf.concat([Z, y_1],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "W1_d = tf.get_variable(name='weight1_d', shape=[2, 16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2_d = tf.get_variable(name='weight2_d', shape=[16, 32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3_d = tf.get_variable(name='weight3_d', shape=[32, 64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4_d_u = tf.get_variable(name='weight4_d_u', shape=[64, output_dim], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "b1_d = tf.Variable(tf.random_normal([16]), name='bias1_d')\n",
    "b2_d = tf.Variable(tf.random_normal([32]), name='bias2_d')\n",
    "b3_d = tf.Variable(tf.random_normal([64]), name='bias3_d')\n",
    "b4_d_u = tf.Variable(tf.random_normal([output_dim]), name='bias4_d_u')\n",
    "\n",
    "\n",
    "L2_d = tf.matmul(Z, W1_d) + b1_d\n",
    "L3_d = tf.matmul(L2_d, W2_d) + b2_d\n",
    "L4_d = tf.matmul(L3_d, W3_d) + b3_d\n",
    "L5_d_u = tf.matmul(L4_d, W4_d_u) + b4_d_u\n",
    "\n",
    "one_ = tf.ones([batch_size,8])\n",
    "\n",
    "eps_d = tf.random_normal(([batch_size, 8]), 0, 1, dtype=tf.float32)\n",
    "X_ = tf.add(L5_d_u[:,:8], tf.multiply(tf.sqrt(tf.exp(L5_d_u[:,8:])), eps_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_i_1 = tf.fill([batch_size, 1],3.0432)\n",
    "Cov_i_2 = tf.fill([batch_size, 1],-2.49357)\n",
    "Cov_i_3 = tf.fill([batch_size, 1],-2.49357)\n",
    "Cov_i_4 = tf.fill([batch_size, 1],3.0432)\n",
    "Cov_inv_b =  tf.concat([Cov_i_1, Cov_i_2, Cov_i_3, Cov_i_4], 1)\n",
    "Cov_inv_b = tf.reshape(Cov_inv_b, shape=[batch_size, 2, 2])\n",
    "trr = tf.matmul(Cov_inv_b,tf.matmul(Sigma_mat,Sigma_mat))\n",
    "\n",
    "KL_loss_1 = tf.trace(trr)\n",
    "condition = tf.less(KL_loss_1, 0.)\n",
    "KL_loss_1  = tf.where(condition, tf.math.negative(KL_loss_1), KL_loss_1)\n",
    "\n",
    "Cov_inv = tf.constant([[3.0432, -2.49357], \n",
    "                       [-2.49357, 3.0432]])\n",
    "L5_U1 = tf.fill([batch_size, 1],0.)\n",
    "L5_U2 = tf.fill([batch_size, 1],0.)\n",
    "L5_Uo = tf.concat([L5_U1, L5_U2], 1)\n",
    "\n",
    "KL_loss_2 = tf.reduce_sum(tf.multiply(tf.matmul((L5_Uo - L5_u[:,0:2]),Cov_inv),(L5_Uo - L5_u[:,0:2])),1)\n",
    "\n",
    "Cov = tf.constant([[1., 0.819389255], \n",
    "                   [0.819389255, 1.]])\n",
    "det = tf.linalg.det(Cov)\n",
    "det_cov_up = tf.fill([batch_size],det)\n",
    "\n",
    "det_cov_down = tf.linalg.det(tf.matmul(Sigma_mat,Sigma_mat))\n",
    "\n",
    "condition = tf.less(det_cov_down, 0.)\n",
    "dum = tf.ones([batch_size])\n",
    "det_cov_down_  = tf.where(condition, dum*0.0000000001, det_cov_down)\n",
    "KL_loss_3 = tf.math.log(tf.div_no_nan(det_cov_up, det_cov_down_))\n",
    "        \n",
    "KL_loss = KL_loss_1 + KL_loss_2 + KL_loss_3\n",
    "\n",
    "Repro_loss = tf.reduce_sum(0.5 * tf.math.log(tf.exp(L5_d_u[:,8:]))\n",
    "                   + tf.div_no_nan(tf.square(X-L5_d_u[:,:8]), 2.0 * tf.exp(L5_d_u[:,8:])), 1)\n",
    "\n",
    "cost =  tf.reduce_mean(Repro_loss) + tf.reduce_mean(KL_loss - 2)*0.5\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate_).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    print(\"\\nStart training\")\n",
    "    sess.run(init)\n",
    "    batch_size_ = 500\n",
    "    for epoch in range(1000):\n",
    "        total_batch = int(data_size / batch_size_)\n",
    "        if epoch < 500:\n",
    "            LR = 0.01\n",
    "        if epoch > 500:\n",
    "            LR = 0.01\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x = x_data_training[i*batch_size_:(i+1)*batch_size_]\n",
    "            _, d, Z_val = sess.run((optimizer, cost, L5_u), feed_dict={X: batch_x, batch_size: batch_size_, learning_rate_: LR})\n",
    "    \n",
    "            if i % 50 == 0:\n",
    "                print(\"Epoch #%d step=%d cost=%f\" % (epoch, i, d))\n",
    "   \n",
    "    save_path = saver.save(sess, \"checkpoint path\") \n",
    "    Z_val, Z_, X_mean_val, X_d, eps_ = sess.run((L5_u, Z, L5_d_u, X_, eps_d), feed_dict={X: x_data_training[:data_size,:], batch_size: data_size, learning_rate_: LR})\n",
    "\n",
    "    plt.plot( Z_[:,0], Z_[:,1], '+')\n",
    "    plt.plot(y_data_training[:data_size,0], y_data_training[:data_size, 1], '.')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(7):\n",
    "    plt.plot(X_d[:,0], X_d[:,i+1], '+')\n",
    "    plt.plot(x_data_training[:data_size,0], x_data_training[:data_size,i+1], '*')\n",
    "    plt.show()\n",
    "for i in range(6):\n",
    "    plt.plot(X_d[:,1], X_d[:,i+2], '+')\n",
    "    plt.plot(x_data_training[:data_size,1], x_data_training[:data_size,i+2], '*')\n",
    "    plt.show()\n",
    "for i in range(5):\n",
    "    plt.plot(X_d[:,2], X_d[:,i+3], '+')\n",
    "    plt.plot(x_data_training[:data_size,2], x_data_training[:data_size,i+3], '*')\n",
    "    plt.show()\n",
    "for i in range(4):\n",
    "    plt.plot(X_d[:,3], X_d[:,i+4], '+')\n",
    "    plt.plot(x_data_training[:data_size,3], x_data_training[:data_size,i+4], '*')\n",
    "    plt.show()\n",
    "for i in range(3):\n",
    "    plt.plot(X_d[:,4], X_d[:,i+5], '+')\n",
    "    plt.plot(x_data_training[:data_size,4], x_data_training[:data_size,i+5], '*')\n",
    "    plt.show()\n",
    "for i in range(2):\n",
    "    plt.plot(X_d[:,5], X_d[:,i+6], '+')\n",
    "    plt.plot(x_data_training[:data_size,5], x_data_training[:data_size,i+6], '*')\n",
    "    plt.show()\n",
    "for i in range(1):\n",
    "    plt.plot(X_d[:,6], X_d[:,i+7], '+')\n",
    "    plt.plot(x_data_training[:data_size,6], x_data_training[:data_size,i+7], '*')\n",
    "    plt.show()\n",
    "for i in range(0):\n",
    "    plt.plot(X_d[:,7], X_d[:,i+8], '+')\n",
    "    plt.plot(x_data_training[:data_size,7], x_data_training[:data_size,i+8], '*')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
